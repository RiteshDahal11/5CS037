{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aQ6-sStrGXc9"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_function(x):\n",
        "  \"\"\"\n",
        "  Computes the logistic function applied to any value of x.\n",
        "  Arguments:\n",
        "  x: scalar or numpy array of any size.\n",
        "  Returns:\n",
        "  y: logistic function applied to x.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  y = 1/(1+np.exp(-x))\n",
        "  return y"
      ],
      "metadata": {
        "id": "J2GhE_p4Ge-p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_logistic_function():\n",
        "  \"\"\"\n",
        "  Test cases for the logistic_function.\n",
        "  \"\"\"\n",
        "  # Test with scalar input\n",
        "  x_scalar = 0\n",
        "  expected_output_scalar = round(1 / (1 + np.exp(0)), 3) # Expected output: 0.5\n",
        "  assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "  # Test with positive scalar input\n",
        "  x_pos = 2\n",
        "  expected_output_pos = round(1 / (1 + np.exp(-2)), 3) # Expected output: ~0.881\n",
        "  assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "  # Test with negative scalar input\n",
        "  x_neg = -3\n",
        "  expected_output_neg = round(1 / (1 + np.exp(3)), 3) # Expected output: ~0.047\n",
        "  assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "  # Test with numpy array input\n",
        "  x_array = np.array([0, 2, -3])\n",
        "  expected_output_array = np.array([0.5, 0.881, 0.047]) # Adjusted expected values rounded to 3 decimals\n",
        "  # Use np.round to round the array element-wise and compare\n",
        "  assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "  print(\"All tests passed!\")\n",
        "  # Run the test case\n",
        "test_logistic_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNpXZRpeGirI",
        "outputId": "43a38fc5-0210-4ade-dd81-0a193181d1c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def log_loss(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for true target value y ={0 or 1} and predicted target value yâ€™ inbetween {0-1}.\n",
        "  Arguments:\n",
        "  y_true (scalar): true target value {0 or 1}.\n",
        "  y_pred (scalar): predicted taget value {0-1}.\n",
        "  Returns:\n",
        "  loss (float): loss/error value\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Ensure y_pred is clipped to avoid log(0)\n",
        "  y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "  loss = -y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "om2tAQ27Gyss"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function:\n",
        "y_true, y_pred = 0, 0.1\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')\n",
        "print(\"+++++++++++++--------------------------++++++++++++++++++++++++\")\n",
        "y_true, y_pred = 1, 0.9\n",
        "print(f'log loss({y_true}, {y_pred}) ==> {log_loss(y_true, y_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_G0v8GmG2pf",
        "outputId": "09b06f18-43dc-4668-99a9-3ee9054adbf3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "log loss(0, 0.1) ==> 0.10536051565782628\n",
            "+++++++++++++--------------------------++++++++++++++++++++++++\n",
            "log loss(1, 0.9) ==> 0.10536051565782628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_log_loss():\n",
        "  \"\"\"\n",
        "  Test cases for the log_loss function.\n",
        "  \"\"\"\n",
        "  import numpy as np\n",
        "  # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "  y_true = 1\n",
        "  y_pred = 1\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction ( y_true=1, y_pred=1)\"\n",
        "  # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "  y_true = 0\n",
        "  y_pred = 0\n",
        "  expected_loss = 0.0 # Log loss is 0 for perfect prediction\n",
        "  assert np.isclose(log_loss(y_true, y_pred), expected_loss), \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
        "  # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0)\n",
        "  y_true = 1\n",
        "  y_pred = 0\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "  y_true = 0\n",
        "  y_pred = 1\n",
        "  try:\n",
        "    log_loss(y_true, y_pred) # This should raise an error due to log(0)\n",
        "  except ValueError:\n",
        "    pass # Test passed if ValueError is raised for log(0)\n",
        "  # Test case 5: Partially correct prediction\n",
        "  y_true = 1\n",
        "  y_pred = 0.8\n",
        "  expected_loss = -(1 * np.log(0.8)) - (0 * np.log(0.2)) # ~0.2231\n",
        "test_log_loss()"
      ],
      "metadata": {
        "id": "jRsfSIJaHABj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cost_function(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "  Args:\n",
        "  y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "  y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "  Returns:\n",
        "  cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "  \"\"\"\n",
        "  assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "  n = len(y_true)\n",
        "  loss_vec = log_loss(y_true, y_pred)\n",
        "  cost = np.sum(loss_vec) / n\n",
        "  return cost"
      ],
      "metadata": {
        "id": "ZjuFYKjZHBBB"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def test_cost_function():\n",
        "  # Test case 1: Simple example with known expected cost\n",
        "  y_true = np.array([1, 0, 1])\n",
        "  y_pred = np.array([0.9, 0.1, 0.8])\n",
        "  # Expected output: Manually calculate cost for these values\n",
        "  # log_loss(y_true, y_pred) for each example\n",
        "  expected_cost = (-(1 * np.log(0.9)) - (1 - 1) * np.log(1 - 0.9) +\n",
        "                  -(0 * np.log(0.1)) - (1 - 0) * np.log(1 - 0.1) +\n",
        "                  -(1 * np.log(0.8)) - (1 - 1) * np.log(1 - 0.8)) / 3\n",
        "\n",
        "  # Call the cost_function to get the result\n",
        "  result = cost_function(y_true, y_pred)\n",
        "  # Assert that the result is close to the expected cost with a tolerance of 1e-6\n",
        "  assert np.isclose(result, expected_cost, atol=1e-6), f\"Test failed: {result} != {expected_cost}\"\n",
        "  print(\"Test passed for simple case!\")\n",
        "  # Run the test case\n",
        "test_cost_function()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iTwYVBlHHAr",
        "outputId": "6ee4fce0-f96c-4833-a20f-c0bff9856d53"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Function to compute cost function in terms of model parameters - using vectorization\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Computes the cost function, given data and model parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (m,n)): data on features, m observations with n features.\n",
        "  y (array_like, shape (m,)): array of true values of target (0 or 1).\n",
        "  w (array_like, shape (n,)): weight parameters of the model.\n",
        "  b (float): bias parameter of the model.\n",
        "  Returns:\n",
        "  cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of feature observations and number of target observations do not match.\"\n",
        "  assert len(w) == d, \"Number of features and number of weight parameters do not match.\"\n",
        "  # Compute z using np.dot\n",
        "  z = np.dot(X, w) + b\n",
        "  # Compute predictions using logistic function (sigmoid)\n",
        "  y_pred = logistic_function(z)\n",
        "  # Compute the cost using the cost function\n",
        "  cost = cost_function(y, y_pred)\n",
        "  return cost\n",
        "  # Testing the Function:\n",
        "X, y, w, b = np.array([[10, 20], [-10, 10]]), np.array([1, 0]), np.array([0.5, 1.5]), 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc0WG4mgHTG5",
        "outputId": "6cf6bb21-e05d-454c-a645-4d9e239373c8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradient(X, y, w, b):\n",
        "  \"\"\"\n",
        "  Computes gradients of the cost function with respect to model parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "  y (array_like, shape (n,)): True labels (0 or 1)\n",
        "  w (array_like, shape (d,)): Weight parameters of the model\n",
        "  b (float): Bias parameter of the model\n",
        "  Returns:\n",
        "  grad_w (array_like, shape (d,)): Gradients of the cost function with respect to the weight\n",
        "  parameters\n",
        "  grad_b (float): Gradient of the cost function with respect to the bias parameter\n",
        "  \"\"\"\n",
        "  n, d = X.shape # X has shape (n, d)\n",
        "  assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "  assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "  # Compute predictions using logistic function (sigmoid)\n",
        "  y_pred = 1 / (1 + np.exp(-(np.dot(X, w) + b)))\n",
        "  # Compute gradients\n",
        "  grad_w = -(1 / n) * np.sum(np.dot(X.T, (y-y_pred)))\n",
        "  grad_b = -(1 / n) * np.sum(y-y_pred)\n",
        "  return grad_w, grad_b"
      ],
      "metadata": {
        "id": "-m9hAyO1HyPD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simple test case\n",
        "X = np.array([[10, 20], [-10, 10]]) # shape (2, 2)\n",
        "y = np.array([1, 0]) # shape (2,)\n",
        "w = np.array([0.5, 1.5]) # shape (2,)\n",
        "b = 1 # scalar\n",
        "# Assertion tests\n",
        "try:\n",
        "  grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "  print(\"Gradients computed successfully.\")\n",
        "  print(f\"grad_w: {grad_w}\")\n",
        "  print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "  print(f\"Assertion error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8RcKoGTHzWO",
        "outputId": "80610bf5-13b9-4be6-b433-1b4089b3fb22"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: -2.6645352591003757e-15\n",
            "grad_b: 0.4999916492890759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "  \"\"\"\n",
        "  Implements batch gradient descent to optimize logistic regression parameters.\n",
        "  Args:\n",
        "  X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "  y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "  w (array_like, shape (d,)): Initial weight parameters\n",
        "  b (float): Initial bias parameter\n",
        "  alpha (float): Learning rate\n",
        "  n_iter (int): Number of iterations\n",
        "  show_cost (bool): If True, displays cost every 100 iterations\n",
        "  show_params (bool): If True, displays parameters every 100 iterations\n",
        "  Returns:\n",
        "  w (array_like, shape (d,)): Optimized weight parameters\n",
        "  b (float): Optimized bias parameter\n",
        "  cost_history (list): List of cost values over iterations\n",
        "  params_history (list): List of parameters (w, b) over iterations\n",
        "  \"\"\"\n",
        "  n, d = X.shape\n",
        "  assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "  assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "  cost_history = []\n",
        "  params_history = []\n",
        "  for i in range(n_iter):\n",
        "    # Compute gradients\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    # Update weights and bias\n",
        "    w -= alpha * grad_w\n",
        "    b -= alpha * grad_b\n",
        "    # Compute cost\n",
        "    cost = costfunction_logreg(X, y, w, b)\n",
        "    # Store cost and parameters\n",
        "    cost_history.append(cost)\n",
        "    params_history.append((w.copy(), b))\n",
        "    # Optionally print cost and parameters\n",
        "    if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "      print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "    if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "      print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "  return w, b, cost_history, params_history\n",
        "# Test the gradient_descent function with sample data\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "y = np.array([1, 0]) # Shape (2,)\n",
        "w = np.zeros(X.shape[1]) # Shape (2,) - same as number of features\n",
        "b = 0.0 # Scalar\n",
        "alpha = 0.1 # Learning rate\n",
        "n_iter = 100000 # Number of iterations\n",
        "# Perform gradient descent\n",
        "w_out, b_out, cost_history, params_history = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=True,show_params=False)\n",
        "# Print final parameters and cost\n",
        "print(\"\\nFinal parameters:\")\n",
        "print(f\"w: {w_out}, b: {b_out}\")\n",
        "print(f\"Final cost: {cost_history[-1]:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhtqpqgsIJxZ",
        "outputId": "cdf4ff7c-d291-4a4d-e4d5-3d03e588b9f2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0: Cost = 0.692585\n",
            "Iteration 100: Cost = 0.640344\n",
            "Iteration 200: Cost = 0.593739\n",
            "Iteration 300: Cost = 0.551937\n",
            "Iteration 400: Cost = 0.514393\n",
            "Iteration 500: Cost = 0.480617\n",
            "Iteration 600: Cost = 0.450173\n",
            "Iteration 700: Cost = 0.422672\n",
            "Iteration 800: Cost = 0.397773\n",
            "Iteration 900: Cost = 0.375177\n",
            "Iteration 1000: Cost = 0.354621\n",
            "Iteration 1100: Cost = 0.335875\n",
            "Iteration 1200: Cost = 0.318738\n",
            "Iteration 1300: Cost = 0.303035\n",
            "Iteration 1400: Cost = 0.288611\n",
            "Iteration 1500: Cost = 0.275332\n",
            "Iteration 1600: Cost = 0.263079\n",
            "Iteration 1700: Cost = 0.251749\n",
            "Iteration 1800: Cost = 0.241250\n",
            "Iteration 1900: Cost = 0.231501\n",
            "Iteration 2000: Cost = 0.222431\n",
            "Iteration 2100: Cost = 0.213977\n",
            "Iteration 2200: Cost = 0.206082\n",
            "Iteration 2300: Cost = 0.198696\n",
            "Iteration 2400: Cost = 0.191776\n",
            "Iteration 2500: Cost = 0.185281\n",
            "Iteration 2600: Cost = 0.179175\n",
            "Iteration 2700: Cost = 0.173427\n",
            "Iteration 2800: Cost = 0.168008\n",
            "Iteration 2900: Cost = 0.162892\n",
            "Iteration 3000: Cost = 0.158056\n",
            "Iteration 3100: Cost = 0.153479\n",
            "Iteration 3200: Cost = 0.149140\n",
            "Iteration 3300: Cost = 0.145024\n",
            "Iteration 3400: Cost = 0.141114\n",
            "Iteration 3500: Cost = 0.137396\n",
            "Iteration 3600: Cost = 0.133857\n",
            "Iteration 3700: Cost = 0.130484\n",
            "Iteration 3800: Cost = 0.127267\n",
            "Iteration 3900: Cost = 0.124196\n",
            "Iteration 4000: Cost = 0.121261\n",
            "Iteration 4100: Cost = 0.118454\n",
            "Iteration 4200: Cost = 0.115766\n",
            "Iteration 4300: Cost = 0.113192\n",
            "Iteration 4400: Cost = 0.110723\n",
            "Iteration 4500: Cost = 0.108355\n",
            "Iteration 4600: Cost = 0.106080\n",
            "Iteration 4700: Cost = 0.103895\n",
            "Iteration 4800: Cost = 0.101793\n",
            "Iteration 4900: Cost = 0.099771\n",
            "Iteration 5000: Cost = 0.097823\n",
            "Iteration 5100: Cost = 0.095947\n",
            "Iteration 5200: Cost = 0.094138\n",
            "Iteration 5300: Cost = 0.092393\n",
            "Iteration 5400: Cost = 0.090709\n",
            "Iteration 5500: Cost = 0.089083\n",
            "Iteration 5600: Cost = 0.087511\n",
            "Iteration 5700: Cost = 0.085992\n",
            "Iteration 5800: Cost = 0.084522\n",
            "Iteration 5900: Cost = 0.083100\n",
            "Iteration 6000: Cost = 0.081723\n",
            "Iteration 6100: Cost = 0.080389\n",
            "Iteration 6200: Cost = 0.079097\n",
            "Iteration 6300: Cost = 0.077843\n",
            "Iteration 6400: Cost = 0.076628\n",
            "Iteration 6500: Cost = 0.075448\n",
            "Iteration 6600: Cost = 0.074303\n",
            "Iteration 6700: Cost = 0.073191\n",
            "Iteration 6800: Cost = 0.072110\n",
            "Iteration 6900: Cost = 0.071060\n",
            "Iteration 7000: Cost = 0.070039\n",
            "Iteration 7100: Cost = 0.069046\n",
            "Iteration 7200: Cost = 0.068080\n",
            "Iteration 7300: Cost = 0.067139\n",
            "Iteration 7400: Cost = 0.066223\n",
            "Iteration 7500: Cost = 0.065332\n",
            "Iteration 7600: Cost = 0.064463\n",
            "Iteration 7700: Cost = 0.063616\n",
            "Iteration 7800: Cost = 0.062791\n",
            "Iteration 7900: Cost = 0.061986\n",
            "Iteration 8000: Cost = 0.061201\n",
            "Iteration 8100: Cost = 0.060434\n",
            "Iteration 8200: Cost = 0.059687\n",
            "Iteration 8300: Cost = 0.058957\n",
            "Iteration 8400: Cost = 0.058244\n",
            "Iteration 8500: Cost = 0.057548\n",
            "Iteration 8600: Cost = 0.056867\n",
            "Iteration 8700: Cost = 0.056202\n",
            "Iteration 8800: Cost = 0.055553\n",
            "Iteration 8900: Cost = 0.054917\n",
            "Iteration 9000: Cost = 0.054296\n",
            "Iteration 9100: Cost = 0.053688\n",
            "Iteration 9200: Cost = 0.053093\n",
            "Iteration 9300: Cost = 0.052511\n",
            "Iteration 9400: Cost = 0.051941\n",
            "Iteration 9500: Cost = 0.051383\n",
            "Iteration 9600: Cost = 0.050837\n",
            "Iteration 9700: Cost = 0.050302\n",
            "Iteration 9800: Cost = 0.049778\n",
            "Iteration 9900: Cost = 0.049264\n",
            "Iteration 10000: Cost = 0.048761\n",
            "Iteration 10100: Cost = 0.048267\n",
            "Iteration 10200: Cost = 0.047783\n",
            "Iteration 10300: Cost = 0.047309\n",
            "Iteration 10400: Cost = 0.046844\n",
            "Iteration 10500: Cost = 0.046387\n",
            "Iteration 10600: Cost = 0.045939\n",
            "Iteration 10700: Cost = 0.045500\n",
            "Iteration 10800: Cost = 0.045068\n",
            "Iteration 10900: Cost = 0.044645\n",
            "Iteration 11000: Cost = 0.044229\n",
            "Iteration 11100: Cost = 0.043821\n",
            "Iteration 11200: Cost = 0.043420\n",
            "Iteration 11300: Cost = 0.043026\n",
            "Iteration 11400: Cost = 0.042640\n",
            "Iteration 11500: Cost = 0.042259\n",
            "Iteration 11600: Cost = 0.041886\n",
            "Iteration 11700: Cost = 0.041519\n",
            "Iteration 11800: Cost = 0.041158\n",
            "Iteration 11900: Cost = 0.040803\n",
            "Iteration 12000: Cost = 0.040454\n",
            "Iteration 12100: Cost = 0.040111\n",
            "Iteration 12200: Cost = 0.039774\n",
            "Iteration 12300: Cost = 0.039442\n",
            "Iteration 12400: Cost = 0.039116\n",
            "Iteration 12500: Cost = 0.038794\n",
            "Iteration 12600: Cost = 0.038478\n",
            "Iteration 12700: Cost = 0.038167\n",
            "Iteration 12800: Cost = 0.037861\n",
            "Iteration 12900: Cost = 0.037560\n",
            "Iteration 13000: Cost = 0.037263\n",
            "Iteration 13100: Cost = 0.036971\n",
            "Iteration 13200: Cost = 0.036683\n",
            "Iteration 13300: Cost = 0.036400\n",
            "Iteration 13400: Cost = 0.036121\n",
            "Iteration 13500: Cost = 0.035846\n",
            "Iteration 13600: Cost = 0.035575\n",
            "Iteration 13700: Cost = 0.035308\n",
            "Iteration 13800: Cost = 0.035045\n",
            "Iteration 13900: Cost = 0.034786\n",
            "Iteration 14000: Cost = 0.034531\n",
            "Iteration 14100: Cost = 0.034279\n",
            "Iteration 14200: Cost = 0.034031\n",
            "Iteration 14300: Cost = 0.033787\n",
            "Iteration 14400: Cost = 0.033545\n",
            "Iteration 14500: Cost = 0.033308\n",
            "Iteration 14600: Cost = 0.033073\n",
            "Iteration 14700: Cost = 0.032842\n",
            "Iteration 14800: Cost = 0.032614\n",
            "Iteration 14900: Cost = 0.032389\n",
            "Iteration 15000: Cost = 0.032167\n",
            "Iteration 15100: Cost = 0.031948\n",
            "Iteration 15200: Cost = 0.031732\n",
            "Iteration 15300: Cost = 0.031518\n",
            "Iteration 15400: Cost = 0.031308\n",
            "Iteration 15500: Cost = 0.031100\n",
            "Iteration 15600: Cost = 0.030895\n",
            "Iteration 15700: Cost = 0.030693\n",
            "Iteration 15800: Cost = 0.030493\n",
            "Iteration 15900: Cost = 0.030296\n",
            "Iteration 16000: Cost = 0.030101\n",
            "Iteration 16100: Cost = 0.029909\n",
            "Iteration 16200: Cost = 0.029719\n",
            "Iteration 16300: Cost = 0.029531\n",
            "Iteration 16400: Cost = 0.029346\n",
            "Iteration 16500: Cost = 0.029163\n",
            "Iteration 16600: Cost = 0.028983\n",
            "Iteration 16700: Cost = 0.028804\n",
            "Iteration 16800: Cost = 0.028628\n",
            "Iteration 16900: Cost = 0.028454\n",
            "Iteration 17000: Cost = 0.028282\n",
            "Iteration 17100: Cost = 0.028111\n",
            "Iteration 17200: Cost = 0.027943\n",
            "Iteration 17300: Cost = 0.027777\n",
            "Iteration 17400: Cost = 0.027613\n",
            "Iteration 17500: Cost = 0.027451\n",
            "Iteration 17600: Cost = 0.027290\n",
            "Iteration 17700: Cost = 0.027132\n",
            "Iteration 17800: Cost = 0.026975\n",
            "Iteration 17900: Cost = 0.026820\n",
            "Iteration 18000: Cost = 0.026667\n",
            "Iteration 18100: Cost = 0.026515\n",
            "Iteration 18200: Cost = 0.026365\n",
            "Iteration 18300: Cost = 0.026217\n",
            "Iteration 18400: Cost = 0.026071\n",
            "Iteration 18500: Cost = 0.025926\n",
            "Iteration 18600: Cost = 0.025782\n",
            "Iteration 18700: Cost = 0.025641\n",
            "Iteration 18800: Cost = 0.025500\n",
            "Iteration 18900: Cost = 0.025362\n",
            "Iteration 19000: Cost = 0.025224\n",
            "Iteration 19100: Cost = 0.025089\n",
            "Iteration 19200: Cost = 0.024954\n",
            "Iteration 19300: Cost = 0.024821\n",
            "Iteration 19400: Cost = 0.024690\n",
            "Iteration 19500: Cost = 0.024560\n",
            "Iteration 19600: Cost = 0.024431\n",
            "Iteration 19700: Cost = 0.024303\n",
            "Iteration 19800: Cost = 0.024177\n",
            "Iteration 19900: Cost = 0.024052\n",
            "Iteration 20000: Cost = 0.023928\n",
            "Iteration 20100: Cost = 0.023806\n",
            "Iteration 20200: Cost = 0.023685\n",
            "Iteration 20300: Cost = 0.023565\n",
            "Iteration 20400: Cost = 0.023446\n",
            "Iteration 20500: Cost = 0.023329\n",
            "Iteration 20600: Cost = 0.023212\n",
            "Iteration 20700: Cost = 0.023097\n",
            "Iteration 20800: Cost = 0.022983\n",
            "Iteration 20900: Cost = 0.022870\n",
            "Iteration 21000: Cost = 0.022758\n",
            "Iteration 21100: Cost = 0.022647\n",
            "Iteration 21200: Cost = 0.022537\n",
            "Iteration 21300: Cost = 0.022428\n",
            "Iteration 21400: Cost = 0.022321\n",
            "Iteration 21500: Cost = 0.022214\n",
            "Iteration 21600: Cost = 0.022108\n",
            "Iteration 21700: Cost = 0.022003\n",
            "Iteration 21800: Cost = 0.021900\n",
            "Iteration 21900: Cost = 0.021797\n",
            "Iteration 22000: Cost = 0.021695\n",
            "Iteration 22100: Cost = 0.021594\n",
            "Iteration 22200: Cost = 0.021494\n",
            "Iteration 22300: Cost = 0.021395\n",
            "Iteration 22400: Cost = 0.021297\n",
            "Iteration 22500: Cost = 0.021200\n",
            "Iteration 22600: Cost = 0.021103\n",
            "Iteration 22700: Cost = 0.021008\n",
            "Iteration 22800: Cost = 0.020913\n",
            "Iteration 22900: Cost = 0.020819\n",
            "Iteration 23000: Cost = 0.020726\n",
            "Iteration 23100: Cost = 0.020634\n",
            "Iteration 23200: Cost = 0.020543\n",
            "Iteration 23300: Cost = 0.020452\n",
            "Iteration 23400: Cost = 0.020362\n",
            "Iteration 23500: Cost = 0.020273\n",
            "Iteration 23600: Cost = 0.020185\n",
            "Iteration 23700: Cost = 0.020098\n",
            "Iteration 23800: Cost = 0.020011\n",
            "Iteration 23900: Cost = 0.019925\n",
            "Iteration 24000: Cost = 0.019840\n",
            "Iteration 24100: Cost = 0.019755\n",
            "Iteration 24200: Cost = 0.019671\n",
            "Iteration 24300: Cost = 0.019588\n",
            "Iteration 24400: Cost = 0.019506\n",
            "Iteration 24500: Cost = 0.019424\n",
            "Iteration 24600: Cost = 0.019343\n",
            "Iteration 24700: Cost = 0.019262\n",
            "Iteration 24800: Cost = 0.019183\n",
            "Iteration 24900: Cost = 0.019103\n",
            "Iteration 25000: Cost = 0.019025\n",
            "Iteration 25100: Cost = 0.018947\n",
            "Iteration 25200: Cost = 0.018870\n",
            "Iteration 25300: Cost = 0.018793\n",
            "Iteration 25400: Cost = 0.018717\n",
            "Iteration 25500: Cost = 0.018642\n",
            "Iteration 25600: Cost = 0.018567\n",
            "Iteration 25700: Cost = 0.018493\n",
            "Iteration 25800: Cost = 0.018419\n",
            "Iteration 25900: Cost = 0.018346\n",
            "Iteration 26000: Cost = 0.018274\n",
            "Iteration 26100: Cost = 0.018202\n",
            "Iteration 26200: Cost = 0.018131\n",
            "Iteration 26300: Cost = 0.018060\n",
            "Iteration 26400: Cost = 0.017990\n",
            "Iteration 26500: Cost = 0.017920\n",
            "Iteration 26600: Cost = 0.017851\n",
            "Iteration 26700: Cost = 0.017782\n",
            "Iteration 26800: Cost = 0.017714\n",
            "Iteration 26900: Cost = 0.017647\n",
            "Iteration 27000: Cost = 0.017580\n",
            "Iteration 27100: Cost = 0.017513\n",
            "Iteration 27200: Cost = 0.017447\n",
            "Iteration 27300: Cost = 0.017381\n",
            "Iteration 27400: Cost = 0.017316\n",
            "Iteration 27500: Cost = 0.017252\n",
            "Iteration 27600: Cost = 0.017188\n",
            "Iteration 27700: Cost = 0.017124\n",
            "Iteration 27800: Cost = 0.017061\n",
            "Iteration 27900: Cost = 0.016998\n",
            "Iteration 28000: Cost = 0.016936\n",
            "Iteration 28100: Cost = 0.016874\n",
            "Iteration 28200: Cost = 0.016812\n",
            "Iteration 28300: Cost = 0.016752\n",
            "Iteration 28400: Cost = 0.016691\n",
            "Iteration 28500: Cost = 0.016631\n",
            "Iteration 28600: Cost = 0.016571\n",
            "Iteration 28700: Cost = 0.016512\n",
            "Iteration 28800: Cost = 0.016453\n",
            "Iteration 28900: Cost = 0.016395\n",
            "Iteration 29000: Cost = 0.016337\n",
            "Iteration 29100: Cost = 0.016279\n",
            "Iteration 29200: Cost = 0.016222\n",
            "Iteration 29300: Cost = 0.016165\n",
            "Iteration 29400: Cost = 0.016109\n",
            "Iteration 29500: Cost = 0.016053\n",
            "Iteration 29600: Cost = 0.015997\n",
            "Iteration 29700: Cost = 0.015942\n",
            "Iteration 29800: Cost = 0.015887\n",
            "Iteration 29900: Cost = 0.015833\n",
            "Iteration 30000: Cost = 0.015779\n",
            "Iteration 30100: Cost = 0.015725\n",
            "Iteration 30200: Cost = 0.015672\n",
            "Iteration 30300: Cost = 0.015619\n",
            "Iteration 30400: Cost = 0.015566\n",
            "Iteration 30500: Cost = 0.015514\n",
            "Iteration 30600: Cost = 0.015462\n",
            "Iteration 30700: Cost = 0.015410\n",
            "Iteration 30800: Cost = 0.015359\n",
            "Iteration 30900: Cost = 0.015308\n",
            "Iteration 31000: Cost = 0.015257\n",
            "Iteration 31100: Cost = 0.015207\n",
            "Iteration 31200: Cost = 0.015157\n",
            "Iteration 31300: Cost = 0.015107\n",
            "Iteration 31400: Cost = 0.015058\n",
            "Iteration 31500: Cost = 0.015009\n",
            "Iteration 31600: Cost = 0.014960\n",
            "Iteration 31700: Cost = 0.014912\n",
            "Iteration 31800: Cost = 0.014864\n",
            "Iteration 31900: Cost = 0.014816\n",
            "Iteration 32000: Cost = 0.014769\n",
            "Iteration 32100: Cost = 0.014721\n",
            "Iteration 32200: Cost = 0.014675\n",
            "Iteration 32300: Cost = 0.014628\n",
            "Iteration 32400: Cost = 0.014582\n",
            "Iteration 32500: Cost = 0.014536\n",
            "Iteration 32600: Cost = 0.014490\n",
            "Iteration 32700: Cost = 0.014445\n",
            "Iteration 32800: Cost = 0.014400\n",
            "Iteration 32900: Cost = 0.014355\n",
            "Iteration 33000: Cost = 0.014310\n",
            "Iteration 33100: Cost = 0.014266\n",
            "Iteration 33200: Cost = 0.014222\n",
            "Iteration 33300: Cost = 0.014178\n",
            "Iteration 33400: Cost = 0.014135\n",
            "Iteration 33500: Cost = 0.014092\n",
            "Iteration 33600: Cost = 0.014049\n",
            "Iteration 33700: Cost = 0.014006\n",
            "Iteration 33800: Cost = 0.013963\n",
            "Iteration 33900: Cost = 0.013921\n",
            "Iteration 34000: Cost = 0.013879\n",
            "Iteration 34100: Cost = 0.013838\n",
            "Iteration 34200: Cost = 0.013796\n",
            "Iteration 34300: Cost = 0.013755\n",
            "Iteration 34400: Cost = 0.013714\n",
            "Iteration 34500: Cost = 0.013673\n",
            "Iteration 34600: Cost = 0.013633\n",
            "Iteration 34700: Cost = 0.013593\n",
            "Iteration 34800: Cost = 0.013553\n",
            "Iteration 34900: Cost = 0.013513\n",
            "Iteration 35000: Cost = 0.013473\n",
            "Iteration 35100: Cost = 0.013434\n",
            "Iteration 35200: Cost = 0.013395\n",
            "Iteration 35300: Cost = 0.013356\n",
            "Iteration 35400: Cost = 0.013318\n",
            "Iteration 35500: Cost = 0.013279\n",
            "Iteration 35600: Cost = 0.013241\n",
            "Iteration 35700: Cost = 0.013203\n",
            "Iteration 35800: Cost = 0.013165\n",
            "Iteration 35900: Cost = 0.013128\n",
            "Iteration 36000: Cost = 0.013090\n",
            "Iteration 36100: Cost = 0.013053\n",
            "Iteration 36200: Cost = 0.013016\n",
            "Iteration 36300: Cost = 0.012980\n",
            "Iteration 36400: Cost = 0.012943\n",
            "Iteration 36500: Cost = 0.012907\n",
            "Iteration 36600: Cost = 0.012871\n",
            "Iteration 36700: Cost = 0.012835\n",
            "Iteration 36800: Cost = 0.012799\n",
            "Iteration 36900: Cost = 0.012764\n",
            "Iteration 37000: Cost = 0.012728\n",
            "Iteration 37100: Cost = 0.012693\n",
            "Iteration 37200: Cost = 0.012658\n",
            "Iteration 37300: Cost = 0.012624\n",
            "Iteration 37400: Cost = 0.012589\n",
            "Iteration 37500: Cost = 0.012555\n",
            "Iteration 37600: Cost = 0.012521\n",
            "Iteration 37700: Cost = 0.012487\n",
            "Iteration 37800: Cost = 0.012453\n",
            "Iteration 37900: Cost = 0.012419\n",
            "Iteration 38000: Cost = 0.012386\n",
            "Iteration 38100: Cost = 0.012353\n",
            "Iteration 38200: Cost = 0.012320\n",
            "Iteration 38300: Cost = 0.012287\n",
            "Iteration 38400: Cost = 0.012254\n",
            "Iteration 38500: Cost = 0.012221\n",
            "Iteration 38600: Cost = 0.012189\n",
            "Iteration 38700: Cost = 0.012157\n",
            "Iteration 38800: Cost = 0.012125\n",
            "Iteration 38900: Cost = 0.012093\n",
            "Iteration 39000: Cost = 0.012061\n",
            "Iteration 39100: Cost = 0.012030\n",
            "Iteration 39200: Cost = 0.011998\n",
            "Iteration 39300: Cost = 0.011967\n",
            "Iteration 39400: Cost = 0.011936\n",
            "Iteration 39500: Cost = 0.011905\n",
            "Iteration 39600: Cost = 0.011874\n",
            "Iteration 39700: Cost = 0.011844\n",
            "Iteration 39800: Cost = 0.011813\n",
            "Iteration 39900: Cost = 0.011783\n",
            "Iteration 40000: Cost = 0.011753\n",
            "Iteration 40100: Cost = 0.011723\n",
            "Iteration 40200: Cost = 0.011693\n",
            "Iteration 40300: Cost = 0.011663\n",
            "Iteration 40400: Cost = 0.011634\n",
            "Iteration 40500: Cost = 0.011605\n",
            "Iteration 40600: Cost = 0.011575\n",
            "Iteration 40700: Cost = 0.011546\n",
            "Iteration 40800: Cost = 0.011517\n",
            "Iteration 40900: Cost = 0.011489\n",
            "Iteration 41000: Cost = 0.011460\n",
            "Iteration 41100: Cost = 0.011431\n",
            "Iteration 41200: Cost = 0.011403\n",
            "Iteration 41300: Cost = 0.011375\n",
            "Iteration 41400: Cost = 0.011347\n",
            "Iteration 41500: Cost = 0.011319\n",
            "Iteration 41600: Cost = 0.011291\n",
            "Iteration 41700: Cost = 0.011263\n",
            "Iteration 41800: Cost = 0.011236\n",
            "Iteration 41900: Cost = 0.011208\n",
            "Iteration 42000: Cost = 0.011181\n",
            "Iteration 42100: Cost = 0.011154\n",
            "Iteration 42200: Cost = 0.011127\n",
            "Iteration 42300: Cost = 0.011100\n",
            "Iteration 42400: Cost = 0.011073\n",
            "Iteration 42500: Cost = 0.011047\n",
            "Iteration 42600: Cost = 0.011020\n",
            "Iteration 42700: Cost = 0.010994\n",
            "Iteration 42800: Cost = 0.010968\n",
            "Iteration 42900: Cost = 0.010942\n",
            "Iteration 43000: Cost = 0.010916\n",
            "Iteration 43100: Cost = 0.010890\n",
            "Iteration 43200: Cost = 0.010864\n",
            "Iteration 43300: Cost = 0.010838\n",
            "Iteration 43400: Cost = 0.010813\n",
            "Iteration 43500: Cost = 0.010787\n",
            "Iteration 43600: Cost = 0.010762\n",
            "Iteration 43700: Cost = 0.010737\n",
            "Iteration 43800: Cost = 0.010712\n",
            "Iteration 43900: Cost = 0.010687\n",
            "Iteration 44000: Cost = 0.010662\n",
            "Iteration 44100: Cost = 0.010637\n",
            "Iteration 44200: Cost = 0.010613\n",
            "Iteration 44300: Cost = 0.010588\n",
            "Iteration 44400: Cost = 0.010564\n",
            "Iteration 44500: Cost = 0.010540\n",
            "Iteration 44600: Cost = 0.010516\n",
            "Iteration 44700: Cost = 0.010492\n",
            "Iteration 44800: Cost = 0.010468\n",
            "Iteration 44900: Cost = 0.010444\n",
            "Iteration 45000: Cost = 0.010420\n",
            "Iteration 45100: Cost = 0.010397\n",
            "Iteration 45200: Cost = 0.010373\n",
            "Iteration 45300: Cost = 0.010350\n",
            "Iteration 45400: Cost = 0.010326\n",
            "Iteration 45500: Cost = 0.010303\n",
            "Iteration 45600: Cost = 0.010280\n",
            "Iteration 45700: Cost = 0.010257\n",
            "Iteration 45800: Cost = 0.010234\n",
            "Iteration 45900: Cost = 0.010212\n",
            "Iteration 46000: Cost = 0.010189\n",
            "Iteration 46100: Cost = 0.010166\n",
            "Iteration 46200: Cost = 0.010144\n",
            "Iteration 46300: Cost = 0.010122\n",
            "Iteration 46400: Cost = 0.010099\n",
            "Iteration 46500: Cost = 0.010077\n",
            "Iteration 46600: Cost = 0.010055\n",
            "Iteration 46700: Cost = 0.010033\n",
            "Iteration 46800: Cost = 0.010011\n",
            "Iteration 46900: Cost = 0.009989\n",
            "Iteration 47000: Cost = 0.009968\n",
            "Iteration 47100: Cost = 0.009946\n",
            "Iteration 47200: Cost = 0.009925\n",
            "Iteration 47300: Cost = 0.009903\n",
            "Iteration 47400: Cost = 0.009882\n",
            "Iteration 47500: Cost = 0.009861\n",
            "Iteration 47600: Cost = 0.009840\n",
            "Iteration 47700: Cost = 0.009818\n",
            "Iteration 47800: Cost = 0.009798\n",
            "Iteration 47900: Cost = 0.009777\n",
            "Iteration 48000: Cost = 0.009756\n",
            "Iteration 48100: Cost = 0.009735\n",
            "Iteration 48200: Cost = 0.009715\n",
            "Iteration 48300: Cost = 0.009694\n",
            "Iteration 48400: Cost = 0.009674\n",
            "Iteration 48500: Cost = 0.009653\n",
            "Iteration 48600: Cost = 0.009633\n",
            "Iteration 48700: Cost = 0.009613\n",
            "Iteration 48800: Cost = 0.009593\n",
            "Iteration 48900: Cost = 0.009573\n",
            "Iteration 49000: Cost = 0.009553\n",
            "Iteration 49100: Cost = 0.009533\n",
            "Iteration 49200: Cost = 0.009513\n",
            "Iteration 49300: Cost = 0.009493\n",
            "Iteration 49400: Cost = 0.009474\n",
            "Iteration 49500: Cost = 0.009454\n",
            "Iteration 49600: Cost = 0.009435\n",
            "Iteration 49700: Cost = 0.009416\n",
            "Iteration 49800: Cost = 0.009396\n",
            "Iteration 49900: Cost = 0.009377\n",
            "Iteration 50000: Cost = 0.009358\n",
            "Iteration 50100: Cost = 0.009339\n",
            "Iteration 50200: Cost = 0.009320\n",
            "Iteration 50300: Cost = 0.009301\n",
            "Iteration 50400: Cost = 0.009282\n",
            "Iteration 50500: Cost = 0.009263\n",
            "Iteration 50600: Cost = 0.009245\n",
            "Iteration 50700: Cost = 0.009226\n",
            "Iteration 50800: Cost = 0.009208\n",
            "Iteration 50900: Cost = 0.009189\n",
            "Iteration 51000: Cost = 0.009171\n",
            "Iteration 51100: Cost = 0.009152\n",
            "Iteration 51200: Cost = 0.009134\n",
            "Iteration 51300: Cost = 0.009116\n",
            "Iteration 51400: Cost = 0.009098\n",
            "Iteration 51500: Cost = 0.009080\n",
            "Iteration 51600: Cost = 0.009062\n",
            "Iteration 51700: Cost = 0.009044\n",
            "Iteration 51800: Cost = 0.009026\n",
            "Iteration 51900: Cost = 0.009009\n",
            "Iteration 52000: Cost = 0.008991\n",
            "Iteration 52100: Cost = 0.008973\n",
            "Iteration 52200: Cost = 0.008956\n",
            "Iteration 52300: Cost = 0.008938\n",
            "Iteration 52400: Cost = 0.008921\n",
            "Iteration 52500: Cost = 0.008904\n",
            "Iteration 52600: Cost = 0.008886\n",
            "Iteration 52700: Cost = 0.008869\n",
            "Iteration 52800: Cost = 0.008852\n",
            "Iteration 52900: Cost = 0.008835\n",
            "Iteration 53000: Cost = 0.008818\n",
            "Iteration 53100: Cost = 0.008801\n",
            "Iteration 53200: Cost = 0.008784\n",
            "Iteration 53300: Cost = 0.008767\n",
            "Iteration 53400: Cost = 0.008751\n",
            "Iteration 53500: Cost = 0.008734\n",
            "Iteration 53600: Cost = 0.008717\n",
            "Iteration 53700: Cost = 0.008701\n",
            "Iteration 53800: Cost = 0.008684\n",
            "Iteration 53900: Cost = 0.008668\n",
            "Iteration 54000: Cost = 0.008652\n",
            "Iteration 54100: Cost = 0.008635\n",
            "Iteration 54200: Cost = 0.008619\n",
            "Iteration 54300: Cost = 0.008603\n",
            "Iteration 54400: Cost = 0.008587\n",
            "Iteration 54500: Cost = 0.008571\n",
            "Iteration 54600: Cost = 0.008555\n",
            "Iteration 54700: Cost = 0.008539\n",
            "Iteration 54800: Cost = 0.008523\n",
            "Iteration 54900: Cost = 0.008507\n",
            "Iteration 55000: Cost = 0.008491\n",
            "Iteration 55100: Cost = 0.008476\n",
            "Iteration 55200: Cost = 0.008460\n",
            "Iteration 55300: Cost = 0.008444\n",
            "Iteration 55400: Cost = 0.008429\n",
            "Iteration 55500: Cost = 0.008413\n",
            "Iteration 55600: Cost = 0.008398\n",
            "Iteration 55700: Cost = 0.008383\n",
            "Iteration 55800: Cost = 0.008367\n",
            "Iteration 55900: Cost = 0.008352\n",
            "Iteration 56000: Cost = 0.008337\n",
            "Iteration 56100: Cost = 0.008322\n",
            "Iteration 56200: Cost = 0.008307\n",
            "Iteration 56300: Cost = 0.008292\n",
            "Iteration 56400: Cost = 0.008277\n",
            "Iteration 56500: Cost = 0.008262\n",
            "Iteration 56600: Cost = 0.008247\n",
            "Iteration 56700: Cost = 0.008232\n",
            "Iteration 56800: Cost = 0.008217\n",
            "Iteration 56900: Cost = 0.008203\n",
            "Iteration 57000: Cost = 0.008188\n",
            "Iteration 57100: Cost = 0.008173\n",
            "Iteration 57200: Cost = 0.008159\n",
            "Iteration 57300: Cost = 0.008144\n",
            "Iteration 57400: Cost = 0.008130\n",
            "Iteration 57500: Cost = 0.008115\n",
            "Iteration 57600: Cost = 0.008101\n",
            "Iteration 57700: Cost = 0.008087\n",
            "Iteration 57800: Cost = 0.008073\n",
            "Iteration 57900: Cost = 0.008058\n",
            "Iteration 58000: Cost = 0.008044\n",
            "Iteration 58100: Cost = 0.008030\n",
            "Iteration 58200: Cost = 0.008016\n",
            "Iteration 58300: Cost = 0.008002\n",
            "Iteration 58400: Cost = 0.007988\n",
            "Iteration 58500: Cost = 0.007974\n",
            "Iteration 58600: Cost = 0.007960\n",
            "Iteration 58700: Cost = 0.007946\n",
            "Iteration 58800: Cost = 0.007933\n",
            "Iteration 58900: Cost = 0.007919\n",
            "Iteration 59000: Cost = 0.007905\n",
            "Iteration 59100: Cost = 0.007892\n",
            "Iteration 59200: Cost = 0.007878\n",
            "Iteration 59300: Cost = 0.007865\n",
            "Iteration 59400: Cost = 0.007851\n",
            "Iteration 59500: Cost = 0.007838\n",
            "Iteration 59600: Cost = 0.007824\n",
            "Iteration 59700: Cost = 0.007811\n",
            "Iteration 59800: Cost = 0.007798\n",
            "Iteration 59900: Cost = 0.007784\n",
            "Iteration 60000: Cost = 0.007771\n",
            "Iteration 60100: Cost = 0.007758\n",
            "Iteration 60200: Cost = 0.007745\n",
            "Iteration 60300: Cost = 0.007732\n",
            "Iteration 60400: Cost = 0.007719\n",
            "Iteration 60500: Cost = 0.007706\n",
            "Iteration 60600: Cost = 0.007693\n",
            "Iteration 60700: Cost = 0.007680\n",
            "Iteration 60800: Cost = 0.007667\n",
            "Iteration 60900: Cost = 0.007654\n",
            "Iteration 61000: Cost = 0.007642\n",
            "Iteration 61100: Cost = 0.007629\n",
            "Iteration 61200: Cost = 0.007616\n",
            "Iteration 61300: Cost = 0.007603\n",
            "Iteration 61400: Cost = 0.007591\n",
            "Iteration 61500: Cost = 0.007578\n",
            "Iteration 61600: Cost = 0.007566\n",
            "Iteration 61700: Cost = 0.007553\n",
            "Iteration 61800: Cost = 0.007541\n",
            "Iteration 61900: Cost = 0.007528\n",
            "Iteration 62000: Cost = 0.007516\n",
            "Iteration 62100: Cost = 0.007504\n",
            "Iteration 62200: Cost = 0.007492\n",
            "Iteration 62300: Cost = 0.007479\n",
            "Iteration 62400: Cost = 0.007467\n",
            "Iteration 62500: Cost = 0.007455\n",
            "Iteration 62600: Cost = 0.007443\n",
            "Iteration 62700: Cost = 0.007431\n",
            "Iteration 62800: Cost = 0.007419\n",
            "Iteration 62900: Cost = 0.007407\n",
            "Iteration 63000: Cost = 0.007395\n",
            "Iteration 63100: Cost = 0.007383\n",
            "Iteration 63200: Cost = 0.007371\n",
            "Iteration 63300: Cost = 0.007359\n",
            "Iteration 63400: Cost = 0.007347\n",
            "Iteration 63500: Cost = 0.007335\n",
            "Iteration 63600: Cost = 0.007324\n",
            "Iteration 63700: Cost = 0.007312\n",
            "Iteration 63800: Cost = 0.007300\n",
            "Iteration 63900: Cost = 0.007289\n",
            "Iteration 64000: Cost = 0.007277\n",
            "Iteration 64100: Cost = 0.007266\n",
            "Iteration 64200: Cost = 0.007254\n",
            "Iteration 64300: Cost = 0.007243\n",
            "Iteration 64400: Cost = 0.007231\n",
            "Iteration 64500: Cost = 0.007220\n",
            "Iteration 64600: Cost = 0.007208\n",
            "Iteration 64700: Cost = 0.007197\n",
            "Iteration 64800: Cost = 0.007186\n",
            "Iteration 64900: Cost = 0.007175\n",
            "Iteration 65000: Cost = 0.007163\n",
            "Iteration 65100: Cost = 0.007152\n",
            "Iteration 65200: Cost = 0.007141\n",
            "Iteration 65300: Cost = 0.007130\n",
            "Iteration 65400: Cost = 0.007119\n",
            "Iteration 65500: Cost = 0.007108\n",
            "Iteration 65600: Cost = 0.007097\n",
            "Iteration 65700: Cost = 0.007086\n",
            "Iteration 65800: Cost = 0.007075\n",
            "Iteration 65900: Cost = 0.007064\n",
            "Iteration 66000: Cost = 0.007053\n",
            "Iteration 66100: Cost = 0.007042\n",
            "Iteration 66200: Cost = 0.007031\n",
            "Iteration 66300: Cost = 0.007020\n",
            "Iteration 66400: Cost = 0.007010\n",
            "Iteration 66500: Cost = 0.006999\n",
            "Iteration 66600: Cost = 0.006988\n",
            "Iteration 66700: Cost = 0.006978\n",
            "Iteration 66800: Cost = 0.006967\n",
            "Iteration 66900: Cost = 0.006956\n",
            "Iteration 67000: Cost = 0.006946\n",
            "Iteration 67100: Cost = 0.006935\n",
            "Iteration 67200: Cost = 0.006925\n",
            "Iteration 67300: Cost = 0.006914\n",
            "Iteration 67400: Cost = 0.006904\n",
            "Iteration 67500: Cost = 0.006894\n",
            "Iteration 67600: Cost = 0.006883\n",
            "Iteration 67700: Cost = 0.006873\n",
            "Iteration 67800: Cost = 0.006863\n",
            "Iteration 67900: Cost = 0.006852\n",
            "Iteration 68000: Cost = 0.006842\n",
            "Iteration 68100: Cost = 0.006832\n",
            "Iteration 68200: Cost = 0.006822\n",
            "Iteration 68300: Cost = 0.006811\n",
            "Iteration 68400: Cost = 0.006801\n",
            "Iteration 68500: Cost = 0.006791\n",
            "Iteration 68600: Cost = 0.006781\n",
            "Iteration 68700: Cost = 0.006771\n",
            "Iteration 68800: Cost = 0.006761\n",
            "Iteration 68900: Cost = 0.006751\n",
            "Iteration 69000: Cost = 0.006741\n",
            "Iteration 69100: Cost = 0.006731\n",
            "Iteration 69200: Cost = 0.006721\n",
            "Iteration 69300: Cost = 0.006712\n",
            "Iteration 69400: Cost = 0.006702\n",
            "Iteration 69500: Cost = 0.006692\n",
            "Iteration 69600: Cost = 0.006682\n",
            "Iteration 69700: Cost = 0.006672\n",
            "Iteration 69800: Cost = 0.006663\n",
            "Iteration 69900: Cost = 0.006653\n",
            "Iteration 70000: Cost = 0.006643\n",
            "Iteration 70100: Cost = 0.006634\n",
            "Iteration 70200: Cost = 0.006624\n",
            "Iteration 70300: Cost = 0.006614\n",
            "Iteration 70400: Cost = 0.006605\n",
            "Iteration 70500: Cost = 0.006595\n",
            "Iteration 70600: Cost = 0.006586\n",
            "Iteration 70700: Cost = 0.006576\n",
            "Iteration 70800: Cost = 0.006567\n",
            "Iteration 70900: Cost = 0.006558\n",
            "Iteration 71000: Cost = 0.006548\n",
            "Iteration 71100: Cost = 0.006539\n",
            "Iteration 71200: Cost = 0.006529\n",
            "Iteration 71300: Cost = 0.006520\n",
            "Iteration 71400: Cost = 0.006511\n",
            "Iteration 71500: Cost = 0.006502\n",
            "Iteration 71600: Cost = 0.006492\n",
            "Iteration 71700: Cost = 0.006483\n",
            "Iteration 71800: Cost = 0.006474\n",
            "Iteration 71900: Cost = 0.006465\n",
            "Iteration 72000: Cost = 0.006456\n",
            "Iteration 72100: Cost = 0.006447\n",
            "Iteration 72200: Cost = 0.006438\n",
            "Iteration 72300: Cost = 0.006429\n",
            "Iteration 72400: Cost = 0.006420\n",
            "Iteration 72500: Cost = 0.006411\n",
            "Iteration 72600: Cost = 0.006402\n",
            "Iteration 72700: Cost = 0.006393\n",
            "Iteration 72800: Cost = 0.006384\n",
            "Iteration 72900: Cost = 0.006375\n",
            "Iteration 73000: Cost = 0.006366\n",
            "Iteration 73100: Cost = 0.006357\n",
            "Iteration 73200: Cost = 0.006348\n",
            "Iteration 73300: Cost = 0.006339\n",
            "Iteration 73400: Cost = 0.006331\n",
            "Iteration 73500: Cost = 0.006322\n",
            "Iteration 73600: Cost = 0.006313\n",
            "Iteration 73700: Cost = 0.006304\n",
            "Iteration 73800: Cost = 0.006296\n",
            "Iteration 73900: Cost = 0.006287\n",
            "Iteration 74000: Cost = 0.006279\n",
            "Iteration 74100: Cost = 0.006270\n",
            "Iteration 74200: Cost = 0.006261\n",
            "Iteration 74300: Cost = 0.006253\n",
            "Iteration 74400: Cost = 0.006244\n",
            "Iteration 74500: Cost = 0.006236\n",
            "Iteration 74600: Cost = 0.006227\n",
            "Iteration 74700: Cost = 0.006219\n",
            "Iteration 74800: Cost = 0.006210\n",
            "Iteration 74900: Cost = 0.006202\n",
            "Iteration 75000: Cost = 0.006193\n",
            "Iteration 75100: Cost = 0.006185\n",
            "Iteration 75200: Cost = 0.006177\n",
            "Iteration 75300: Cost = 0.006168\n",
            "Iteration 75400: Cost = 0.006160\n",
            "Iteration 75500: Cost = 0.006152\n",
            "Iteration 75600: Cost = 0.006144\n",
            "Iteration 75700: Cost = 0.006135\n",
            "Iteration 75800: Cost = 0.006127\n",
            "Iteration 75900: Cost = 0.006119\n",
            "Iteration 76000: Cost = 0.006111\n",
            "Iteration 76100: Cost = 0.006103\n",
            "Iteration 76200: Cost = 0.006094\n",
            "Iteration 76300: Cost = 0.006086\n",
            "Iteration 76400: Cost = 0.006078\n",
            "Iteration 76500: Cost = 0.006070\n",
            "Iteration 76600: Cost = 0.006062\n",
            "Iteration 76700: Cost = 0.006054\n",
            "Iteration 76800: Cost = 0.006046\n",
            "Iteration 76900: Cost = 0.006038\n",
            "Iteration 77000: Cost = 0.006030\n",
            "Iteration 77100: Cost = 0.006022\n",
            "Iteration 77200: Cost = 0.006014\n",
            "Iteration 77300: Cost = 0.006006\n",
            "Iteration 77400: Cost = 0.005998\n",
            "Iteration 77500: Cost = 0.005991\n",
            "Iteration 77600: Cost = 0.005983\n",
            "Iteration 77700: Cost = 0.005975\n",
            "Iteration 77800: Cost = 0.005967\n",
            "Iteration 77900: Cost = 0.005959\n",
            "Iteration 78000: Cost = 0.005952\n",
            "Iteration 78100: Cost = 0.005944\n",
            "Iteration 78200: Cost = 0.005936\n",
            "Iteration 78300: Cost = 0.005928\n",
            "Iteration 78400: Cost = 0.005921\n",
            "Iteration 78500: Cost = 0.005913\n",
            "Iteration 78600: Cost = 0.005905\n",
            "Iteration 78700: Cost = 0.005898\n",
            "Iteration 78800: Cost = 0.005890\n",
            "Iteration 78900: Cost = 0.005883\n",
            "Iteration 79000: Cost = 0.005875\n",
            "Iteration 79100: Cost = 0.005868\n",
            "Iteration 79200: Cost = 0.005860\n",
            "Iteration 79300: Cost = 0.005853\n",
            "Iteration 79400: Cost = 0.005845\n",
            "Iteration 79500: Cost = 0.005838\n",
            "Iteration 79600: Cost = 0.005830\n",
            "Iteration 79700: Cost = 0.005823\n",
            "Iteration 79800: Cost = 0.005815\n",
            "Iteration 79900: Cost = 0.005808\n",
            "Iteration 80000: Cost = 0.005801\n",
            "Iteration 80100: Cost = 0.005793\n",
            "Iteration 80200: Cost = 0.005786\n",
            "Iteration 80300: Cost = 0.005779\n",
            "Iteration 80400: Cost = 0.005771\n",
            "Iteration 80500: Cost = 0.005764\n",
            "Iteration 80600: Cost = 0.005757\n",
            "Iteration 80700: Cost = 0.005749\n",
            "Iteration 80800: Cost = 0.005742\n",
            "Iteration 80900: Cost = 0.005735\n",
            "Iteration 81000: Cost = 0.005728\n",
            "Iteration 81100: Cost = 0.005721\n",
            "Iteration 81200: Cost = 0.005714\n",
            "Iteration 81300: Cost = 0.005706\n",
            "Iteration 81400: Cost = 0.005699\n",
            "Iteration 81500: Cost = 0.005692\n",
            "Iteration 81600: Cost = 0.005685\n",
            "Iteration 81700: Cost = 0.005678\n",
            "Iteration 81800: Cost = 0.005671\n",
            "Iteration 81900: Cost = 0.005664\n",
            "Iteration 82000: Cost = 0.005657\n",
            "Iteration 82100: Cost = 0.005650\n",
            "Iteration 82200: Cost = 0.005643\n",
            "Iteration 82300: Cost = 0.005636\n",
            "Iteration 82400: Cost = 0.005629\n",
            "Iteration 82500: Cost = 0.005622\n",
            "Iteration 82600: Cost = 0.005615\n",
            "Iteration 82700: Cost = 0.005608\n",
            "Iteration 82800: Cost = 0.005602\n",
            "Iteration 82900: Cost = 0.005595\n",
            "Iteration 83000: Cost = 0.005588\n",
            "Iteration 83100: Cost = 0.005581\n",
            "Iteration 83200: Cost = 0.005574\n",
            "Iteration 83300: Cost = 0.005567\n",
            "Iteration 83400: Cost = 0.005561\n",
            "Iteration 83500: Cost = 0.005554\n",
            "Iteration 83600: Cost = 0.005547\n",
            "Iteration 83700: Cost = 0.005540\n",
            "Iteration 83800: Cost = 0.005534\n",
            "Iteration 83900: Cost = 0.005527\n",
            "Iteration 84000: Cost = 0.005520\n",
            "Iteration 84100: Cost = 0.005514\n",
            "Iteration 84200: Cost = 0.005507\n",
            "Iteration 84300: Cost = 0.005500\n",
            "Iteration 84400: Cost = 0.005494\n",
            "Iteration 84500: Cost = 0.005487\n",
            "Iteration 84600: Cost = 0.005481\n",
            "Iteration 84700: Cost = 0.005474\n",
            "Iteration 84800: Cost = 0.005467\n",
            "Iteration 84900: Cost = 0.005461\n",
            "Iteration 85000: Cost = 0.005454\n",
            "Iteration 85100: Cost = 0.005448\n",
            "Iteration 85200: Cost = 0.005441\n",
            "Iteration 85300: Cost = 0.005435\n",
            "Iteration 85400: Cost = 0.005428\n",
            "Iteration 85500: Cost = 0.005422\n",
            "Iteration 85600: Cost = 0.005416\n",
            "Iteration 85700: Cost = 0.005409\n",
            "Iteration 85800: Cost = 0.005403\n",
            "Iteration 85900: Cost = 0.005396\n",
            "Iteration 86000: Cost = 0.005390\n",
            "Iteration 86100: Cost = 0.005384\n",
            "Iteration 86200: Cost = 0.005377\n",
            "Iteration 86300: Cost = 0.005371\n",
            "Iteration 86400: Cost = 0.005365\n",
            "Iteration 86500: Cost = 0.005358\n",
            "Iteration 86600: Cost = 0.005352\n",
            "Iteration 86700: Cost = 0.005346\n",
            "Iteration 86800: Cost = 0.005340\n",
            "Iteration 86900: Cost = 0.005333\n",
            "Iteration 87000: Cost = 0.005327\n",
            "Iteration 87100: Cost = 0.005321\n",
            "Iteration 87200: Cost = 0.005315\n",
            "Iteration 87300: Cost = 0.005309\n",
            "Iteration 87400: Cost = 0.005302\n",
            "Iteration 87500: Cost = 0.005296\n",
            "Iteration 87600: Cost = 0.005290\n",
            "Iteration 87700: Cost = 0.005284\n",
            "Iteration 87800: Cost = 0.005278\n",
            "Iteration 87900: Cost = 0.005272\n",
            "Iteration 88000: Cost = 0.005266\n",
            "Iteration 88100: Cost = 0.005260\n",
            "Iteration 88200: Cost = 0.005254\n",
            "Iteration 88300: Cost = 0.005248\n",
            "Iteration 88400: Cost = 0.005242\n",
            "Iteration 88500: Cost = 0.005236\n",
            "Iteration 88600: Cost = 0.005230\n",
            "Iteration 88700: Cost = 0.005224\n",
            "Iteration 88800: Cost = 0.005218\n",
            "Iteration 88900: Cost = 0.005212\n",
            "Iteration 89000: Cost = 0.005206\n",
            "Iteration 89100: Cost = 0.005200\n",
            "Iteration 89200: Cost = 0.005194\n",
            "Iteration 89300: Cost = 0.005188\n",
            "Iteration 89400: Cost = 0.005182\n",
            "Iteration 89500: Cost = 0.005176\n",
            "Iteration 89600: Cost = 0.005170\n",
            "Iteration 89700: Cost = 0.005165\n",
            "Iteration 89800: Cost = 0.005159\n",
            "Iteration 89900: Cost = 0.005153\n",
            "Iteration 90000: Cost = 0.005147\n",
            "Iteration 90100: Cost = 0.005141\n",
            "Iteration 90200: Cost = 0.005136\n",
            "Iteration 90300: Cost = 0.005130\n",
            "Iteration 90400: Cost = 0.005124\n",
            "Iteration 90500: Cost = 0.005118\n",
            "Iteration 90600: Cost = 0.005113\n",
            "Iteration 90700: Cost = 0.005107\n",
            "Iteration 90800: Cost = 0.005101\n",
            "Iteration 90900: Cost = 0.005095\n",
            "Iteration 91000: Cost = 0.005090\n",
            "Iteration 91100: Cost = 0.005084\n",
            "Iteration 91200: Cost = 0.005078\n",
            "Iteration 91300: Cost = 0.005073\n",
            "Iteration 91400: Cost = 0.005067\n",
            "Iteration 91500: Cost = 0.005062\n",
            "Iteration 91600: Cost = 0.005056\n",
            "Iteration 91700: Cost = 0.005050\n",
            "Iteration 91800: Cost = 0.005045\n",
            "Iteration 91900: Cost = 0.005039\n",
            "Iteration 92000: Cost = 0.005034\n",
            "Iteration 92100: Cost = 0.005028\n",
            "Iteration 92200: Cost = 0.005023\n",
            "Iteration 92300: Cost = 0.005017\n",
            "Iteration 92400: Cost = 0.005012\n",
            "Iteration 92500: Cost = 0.005006\n",
            "Iteration 92600: Cost = 0.005001\n",
            "Iteration 92700: Cost = 0.004995\n",
            "Iteration 92800: Cost = 0.004990\n",
            "Iteration 92900: Cost = 0.004984\n",
            "Iteration 93000: Cost = 0.004979\n",
            "Iteration 93100: Cost = 0.004973\n",
            "Iteration 93200: Cost = 0.004968\n",
            "Iteration 93300: Cost = 0.004963\n",
            "Iteration 93400: Cost = 0.004957\n",
            "Iteration 93500: Cost = 0.004952\n",
            "Iteration 93600: Cost = 0.004946\n",
            "Iteration 93700: Cost = 0.004941\n",
            "Iteration 93800: Cost = 0.004936\n",
            "Iteration 93900: Cost = 0.004930\n",
            "Iteration 94000: Cost = 0.004925\n",
            "Iteration 94100: Cost = 0.004920\n",
            "Iteration 94200: Cost = 0.004915\n",
            "Iteration 94300: Cost = 0.004909\n",
            "Iteration 94400: Cost = 0.004904\n",
            "Iteration 94500: Cost = 0.004899\n",
            "Iteration 94600: Cost = 0.004893\n",
            "Iteration 94700: Cost = 0.004888\n",
            "Iteration 94800: Cost = 0.004883\n",
            "Iteration 94900: Cost = 0.004878\n",
            "Iteration 95000: Cost = 0.004873\n",
            "Iteration 95100: Cost = 0.004867\n",
            "Iteration 95200: Cost = 0.004862\n",
            "Iteration 95300: Cost = 0.004857\n",
            "Iteration 95400: Cost = 0.004852\n",
            "Iteration 95500: Cost = 0.004847\n",
            "Iteration 95600: Cost = 0.004842\n",
            "Iteration 95700: Cost = 0.004836\n",
            "Iteration 95800: Cost = 0.004831\n",
            "Iteration 95900: Cost = 0.004826\n",
            "Iteration 96000: Cost = 0.004821\n",
            "Iteration 96100: Cost = 0.004816\n",
            "Iteration 96200: Cost = 0.004811\n",
            "Iteration 96300: Cost = 0.004806\n",
            "Iteration 96400: Cost = 0.004801\n",
            "Iteration 96500: Cost = 0.004796\n",
            "Iteration 96600: Cost = 0.004791\n",
            "Iteration 96700: Cost = 0.004786\n",
            "Iteration 96800: Cost = 0.004781\n",
            "Iteration 96900: Cost = 0.004776\n",
            "Iteration 97000: Cost = 0.004771\n",
            "Iteration 97100: Cost = 0.004766\n",
            "Iteration 97200: Cost = 0.004761\n",
            "Iteration 97300: Cost = 0.004756\n",
            "Iteration 97400: Cost = 0.004751\n",
            "Iteration 97500: Cost = 0.004746\n",
            "Iteration 97600: Cost = 0.004741\n",
            "Iteration 97700: Cost = 0.004736\n",
            "Iteration 97800: Cost = 0.004731\n",
            "Iteration 97900: Cost = 0.004726\n",
            "Iteration 98000: Cost = 0.004721\n",
            "Iteration 98100: Cost = 0.004717\n",
            "Iteration 98200: Cost = 0.004712\n",
            "Iteration 98300: Cost = 0.004707\n",
            "Iteration 98400: Cost = 0.004702\n",
            "Iteration 98500: Cost = 0.004697\n",
            "Iteration 98600: Cost = 0.004692\n",
            "Iteration 98700: Cost = 0.004688\n",
            "Iteration 98800: Cost = 0.004683\n",
            "Iteration 98900: Cost = 0.004678\n",
            "Iteration 99000: Cost = 0.004673\n",
            "Iteration 99100: Cost = 0.004668\n",
            "Iteration 99200: Cost = 0.004664\n",
            "Iteration 99300: Cost = 0.004659\n",
            "Iteration 99400: Cost = 0.004654\n",
            "Iteration 99500: Cost = 0.004649\n",
            "Iteration 99600: Cost = 0.004645\n",
            "Iteration 99700: Cost = 0.004640\n",
            "Iteration 99800: Cost = 0.004635\n",
            "Iteration 99900: Cost = 0.004630\n",
            "Iteration 99999: Cost = 0.004626\n",
            "\n",
            "Final parameters:\n",
            "w: [35.82687144 35.82687144], b: -5.351917991477784\n",
            "Final cost: 0.004626\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Simple assertion test for gradient_descent\n",
        "def test_gradient_descent():\n",
        "  X = np.array([[0.1, 0.2], [-0.1, 0.1]]) # Shape (2, 2)\n",
        "  y = np.array([1, 0]) # Shape (2,)\n",
        "  w = np.zeros(X.shape[1]) # Shape (2,)\n",
        "  b = 0.0 # Scalar\n",
        "  alpha = 0.1 # Learning rate\n",
        "  n_iter = 100 # Number of iterations\n",
        "  # Run gradient descent\n",
        "  w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False,\n",
        "  show_params=False)\n",
        "  # Assertions\n",
        "  assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "  assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "  assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "  assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "  print(\"All tests passed!\")\n",
        "# Run the test\n",
        "test_gradient_descent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sic6qAeKIRDi",
        "outputId": "2dcb3f1b-35c6-43ad-ac01-d46f185ac211"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting cost over iteration\n",
        "plt.figure(figsize = (9, 6))\n",
        "plt.plot(cost_history)\n",
        "plt.xlabel(\"Iteration\", fontsize = 14)\n",
        "plt.ylabel(\"Cost\", fontsize = 14)\n",
        "plt.title(\"Cost vs Iteration\", fontsize = 14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "mPRCUKtgIUQB",
        "outputId": "4676a619-9043-4537-d65e-5e1835560eb7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 900x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(X, w, b, threshold=0.5):\n",
        "  \"\"\"\n",
        "  Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "  Arguments:\n",
        "  X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d\n",
        "  features.\n",
        "  w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "  b (float): Bias learned via gradient descent.\n",
        "  threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "  Returns:\n",
        "  y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "  \"\"\"\n",
        "  # Compute the predicted probabilities using the logistic function\n",
        "  z = np.dot(X, w) + b\n",
        "  y_test_prob = 1/(1 + np.exp(-z)) # z = wx + b\n",
        "  # Classify based on the threshold\n",
        "  y_pred = np.where(y_test_prob >= threshold, 1, 0)\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "kXNIE6oGIZ61"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_prediction():\n",
        "  X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]]) # Shape (3, 2)\n",
        "  w_test = np.array([1.0, -1.0]) # Shape (2,)\n",
        "  b_test = 0.0 # Scalar bias\n",
        "  threshold = 0.5 # Default threshold\n",
        "  # Updated expected output\n",
        "  expected_output = np.array([0, 1, 1])\n",
        "  # Call the prediction function\n",
        "  y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "  # Assert that the output matches the expected output\n",
        "  assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "  print(\"Test passed!\")\n",
        "test_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNw-UPfZIf-l",
        "outputId": "2e4586d3-9293-4a82-9034-a658be547340"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_classification(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Computes the confusion matrix, precision, recall, and F1-score for binary classification.\n",
        "  Arguments:\n",
        "  y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).\n",
        "  y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).\n",
        "  Returns:\n",
        "  metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.\n",
        "  \"\"\"\n",
        "  # Initialize confusion matrix components\n",
        "  TP = np.sum((y_true == 1) & (y_pred == 1)) # True Positives\n",
        "  TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "  FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "  FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "  # Confusion matrix\n",
        "  confusion_matrix = np.array([[TN, FP],\n",
        "  [FN, TP]])\n",
        "  # Precision, recall, and F1-score\n",
        "  precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
        "  recall = TP / (TP + FN) if (TP + FN) > 0.0 else 0.0\n",
        "  f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0.0 else 0.0\n",
        "  # Metrics dictionary\n",
        "  metrics = {\n",
        "  \"confusion_matrix\": confusion_matrix,\n",
        "  \"precision\": precision,\n",
        "  \"recall\": recall,\n",
        "  \"f1_score\": f1_score\n",
        "  }\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "YFJoUFh-Ik-r"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "y_true = np.array([1, 0, 1, 0, 1])\n",
        "y_pred = np.array([1, 0, 0, 0, 1])\n",
        "\n",
        "metrics = evaluate_classification(y_true, y_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\\n\", metrics[\"confusion_matrix\"])\n",
        "print(\"Precision:\", metrics[\"precision\"])\n",
        "print(\"Recall:\", metrics[\"recall\"])\n",
        "print(\"F1-score:\", metrics[\"f1_score\"])"
      ],
      "metadata": {
        "id": "04xm6yx3IoNi",
        "outputId": "6965f6d8-4920-481c-d31b-837dee922c3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[2 0]\n",
            " [1 2]]\n",
            "Precision: 1.0\n",
            "Recall: 0.6666666666666666\n",
            "F1-score: 0.8\n"
          ]
        }
      ]
    }
  ]
}